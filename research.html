---
layout: page
title: Research
permalink: /research/
---

<font size="+2"><strong> Preprint</strong></font><br>
<strong><a href="https://arxiv.org/abs/2302.10586">Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels</a></strong>, 2023<br><strong>Zebin You</strong>, Yong Zhong, Fan bao, Jiacheng Sun, Chongxuan li, Zhu Jun<br>
<p style="text-align:justify"><b><i>Abstract: </i></b>We propose a three-stage training strategy called dual pseudo training (DPT) for conditional image generation and classification in semi-supervised learning. First, a classifier is trained on partially labeled data and predicts pseudo labels for all data. Second, a conditional generative model is trained on all data with pseudo labels and generates pseudo images given labels. Finally, the classifier is trained on real data augmented by pseudo images with labels. We demonstrate large-scale diffusion models and semi-supervised learners benefit mutually with a few labels via DPT. In particular, on the ImageNet 256x256 generation benchmark, DPT can generate realistic, diverse, and semantically correct images with very few labels. With two (i.e., < 0.2%) and five (i.e., < 0.4%) labels per class, DPT achieves an FID of 3.44 and 3.37 respectively, outperforming strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification benchmarks with one, two, and five labels per class, achieving state-of-the-art top-1 accuracies of 59.0 (+2.8), 69.5 (+3.0), and 73.6 (+1.2) respectively.</p>
<br>
